# ğŸ§  ETL Project â€” API to CSV Pipeline

This project demonstrates a simple **Extract, Transform, Load (ETL)** pipeline built in Python.  
It extracts data from a free public API, cleans and structures it with pandas, and then saves it into a CSV file.

---

## ğŸš€ Features

- **Extract:** Pulls sample data from the [JSONPlaceholder API](https://jsonplaceholder.typicode.com/)
- **Transform:** Cleans and standardizes data using pandas
- **Load:** Saves the processed dataset to a local CSV file
- Modular design (`extract.py`, `transform.py`, `load.py`, `main.py`)
- Ready to extend for databases or cloud storage (S3, GCP, etc.)

---

## ğŸ—‚ï¸ Project Structure

etl_project/
â”‚
â”œâ”€â”€ data/ # Folder for output files
â”œâ”€â”€ extract.py # Handles API extraction
â”œâ”€â”€ transform.py # Cleans and formats data
â”œâ”€â”€ load.py # Writes data to CSV
â”œâ”€â”€ main.py # Orchestrates ETL pipeline
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ venv/ # (Local virtual environment - ignored by Git)

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone this repository
```bash
git clone https://github.com/BALAJI-PAMIDI/etl_project.git
cd etl_project
2ï¸âƒ£ Create and activate a virtual environment
python3 -m venv venv
source venv/bin/activate
3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
â–¶ï¸ Run the ETL Pipeline
python main.py
âœ… Expected output:
ğŸ”¹ Extracting data...
ğŸ”¹ Transforming data...
ğŸ”¹ Loading data...
âœ… Data saved successfully to data/posts.csv
ğŸ‰ ETL pipeline completed successfully!
ğŸ“Š Output Example
The script creates a file:
data/posts.csv
userId	id	title	body
1	1	Sunt Aut Facere Repellat Provident	Quia et suscipit suscipit recusandae...
1	2	Qui Est Esse	Est rerum tempore vitae sequi sint...
ğŸ§© Future Enhancements
Load data into SQLite or PostgreSQL
Automate daily ETL runs using Airflow or cron
Push processed data to AWS S3 or Google Cloud Storage
ğŸ‘¤ Author
Balaji Pamidi
ğŸ’¼ GitHub: BALAJI-PAMIDI
ğŸ“ Boston, MA
ğŸ“§ balajip1818@gmail.com
